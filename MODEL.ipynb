{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"z0CqpKmsK9rC"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"2JEwoJ09GVHv","executionInfo":{"status":"ok","timestamp":1716352812678,"user_tz":-330,"elapsed":2,"user":{"displayName":"Pritam Bikram","userId":"07404765756461708866"}}},"outputs":[],"source":["import pickle as pkl\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import math\n","import os\n","import numpy.linalg as la\n","from input_data import preprocess_data,load_sz_data,load_los_data\n","from tgcn import tgcnCell\n","import time\n","from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n","train_rate =  0.8\n","seq_len = 12\n","output_dim = pre_len = 12\n","batch_size =64\n","lr =0.001\n","training_epoch = 100\n","gru_units =64\n","heads=4\n","data=pd.read_csv(\"/content/drive/MyDrive/MTRLEA/DATA/MTERLA_DATA_0.2_ROOT.csv\")\n","data1=np.mat(data)\n","adj=pd.read_csv(\"/content/drive/MyDrive/MTRLEA/MTERLA_1.csv\")\n","adj=np.mat(adj)\n","adj=adj.astype(np.float32)\n","time_len = data1.shape[0]\n","num_nodes = data1.shape[1]\n","#### normalization\n","max_value = np.max(data1)\n","data1  = data1/max_value\n","trainX, trainY, testX, testY = preprocess_data(data1, time_len, train_rate, seq_len, pre_len)\n","totalbatch = int(trainX.shape[0]/batch_size)\n","training_data_count = len(trainX)\n","\n","def self_attention1(x, weight_att,bias_att):\n","    x1=tf.reshape(x,[-1,gru_units])\n","    h1=tf.matmul(x1,weight_att['w11'])+bias_att['b11']\n","    h2=tf.matmul(x1,weight_att['w12'])+bias_att['b12']\n","    h3=tf.matmul(x1,weight_att['w13'])+bias_att['b13']\n","    h4=tf.matmul(x1,weight_att['w14'])+bias_att['b14']\n","    head=tf.concat([h1, h2,h3,h4], 1)\n","    head=tf.matmul(head,weight_att['head'])+bias_att['bias_head']\n","    NODES_D=tf.reshape(head, [-1, num_nodes])\n","    f1 = tf.matmul(NODES_D, weight_att['w2']) + bias_att['b2']\n","    g1 = tf.matmul(NODES_D, weight_att['w2']) + bias_att['b2']\n","    h1 = tf.matmul(NODES_D, weight_att['w2']) + bias_att['b2']\n","\n","    f11 = tf.reshape(f1, [-1,seq_len])\n","    g11 = tf.reshape(g1, [-1,seq_len])\n","    h11 = tf.reshape(h1, [-1,seq_len])\n","    s_out = g11 * f11\n","    beta_out = tf.nn.softmax(s_out, dim=-1)  # attention map\n","    context_out= tf.expand_dims(beta_out,2) * tf.reshape(head,[-1,seq_len,num_nodes])\n","    context_out = tf.transpose(context_out,perm=[0,2,1])\n","    return context_out, beta_out\n","\n","def TGCN(_X, _weights, _biases):\n","    ###\n","    cell_1 = tgcnCell(w_g_1, b_g_1, w_g_2, b_g_2, w_g_3, b_g_3, w_g_4, b_g_4, w_g_5, b_g_5, w_c_1, b_c_1, w_c_2, b_c_2,w_c_3, b_c_3, w_c_4, b_c_4, w_c_5, b_c_5,gru_units, adj, num_nodes=num_nodes)\n","    cell = tf.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True)\n","    _X = tf.unstack(_X, axis=1)\n","    outputs, states = tf.nn.static_rnn(cell, _X, dtype=tf.float32)\n","    out = tf.concat(outputs, axis=0)\n","    out = tf.reshape(out, shape=[seq_len,-1,num_nodes,gru_units])\n","    out = tf.transpose(out, perm=[1,0,2,3])\n","\n","    last_output,alpha = self_attention1(out, weight_att, bias_att)\n","\n","    output = tf.reshape(last_output,shape=[-1,seq_len])\n","    output = tf.matmul(output, weights['out']) + biases['out']\n","    output = tf.reshape(output,shape=[-1,num_nodes,pre_len])\n","    output = tf.transpose(output, perm=[0,2,1])\n","    output = tf.reshape(output, shape=[-1,num_nodes])\n","\n","    return output, outputs, states, alpha\n","\n","tf.compat.v1.disable_eager_execution()\n","inputs = tf.compat.v1.placeholder(tf.float32, shape=[None, seq_len, num_nodes])\n","labels = tf.compat.v1.placeholder(tf.float32, shape=[None, pre_len, num_nodes])\n","\n","\n","bias_candidate=0.0\n","bias_gate=1.0\n","# # Graph weights\n","\n","weights = {\n","    'out': tf.Variable(tf.random.normal([seq_len, pre_len], mean=1.0), name='weight_o')}\n","biases = {\n","    'out': tf.Variable(tf.random.normal([pre_len]),name='bias_o')}\n","weight_att={\n","    'w11':tf.Variable(tf.random.normal([gru_units,1], stddev=0.1),name='att_w11'),\n","    'w12':tf.Variable(tf.random.normal([gru_units,1], stddev=0.1),name='att_w12'),\n","    'w13':tf.Variable(tf.random.normal([gru_units,1], stddev=0.1),name='att_w13'),\n","    'w14':tf.Variable(tf.random.normal([gru_units,1], stddev=0.1),name='att_w14'),\n","    'w15':tf.Variable(tf.random.normal([gru_units,1], stddev=0.1),name='att_w15'),\n","    'w16':tf.Variable(tf.random.normal([gru_units,1], stddev=0.1),name='att_w16'),\n","\n","    'head':tf.Variable(tf.random.normal([heads,1], stddev=0.1),name='att_head'),\n","    'w2':tf.Variable(tf.random.normal([num_nodes,1], stddev=0.1),name='att_w2')}\n","bias_att = {\n","    'b11': tf.Variable(tf.random.normal([1]),name='att_b11'),\n","    'b12': tf.Variable(tf.random.normal([1]),name='att_b12'),\n","    'b13': tf.Variable(tf.random.normal([1]),name='att_b13'),\n","    'b14': tf.Variable(tf.random.normal([1]),name='att_b14'),\n","    'b15': tf.Variable(tf.random.normal([1]),name='att_b15'),\n","    'b16': tf.Variable(tf.random.normal([1]),name='att_b16'),\n","\n","    'bias_head': tf.Variable(tf.random.normal([1]),name='bias_head'),\n","    'b2': tf.Variable(tf.random.normal([1]),name='att_b2')}\n","\n","\n","\n","w_g_1=tf.compat.v1.get_variable('weights1', [1, 2],initializer=tf.initializers.glorot_uniform())\n","b_g_1=tf.compat.v1.get_variable(\"biases1\", [2], initializer=tf.constant_initializer(bias_gate))\n","w_g_2=tf.compat.v1.get_variable('weights2', [2,4],initializer=tf.initializers.glorot_uniform())\n","b_g_2=tf.compat.v1.get_variable(\"biases2\", [4], initializer=tf.constant_initializer(bias_gate))\n","w_g_3=tf.compat.v1.get_variable('weights3', [5,8],initializer=tf.initializers.glorot_uniform())\n","b_g_3=tf.compat.v1.get_variable(\"biases3\", [8], initializer=tf.constant_initializer(bias_gate))\n","w_g_4=tf.compat.v1.get_variable('weights4', [8, 16], initializer=tf.initializers.glorot_uniform())\n","b_g_4=tf.compat.v1.get_variable(\"biases4\", [16], initializer=tf.constant_initializer(bias_gate))\n","w_g_5=tf.compat.v1.get_variable('weights5', [21+gru_units, 2*gru_units], initializer=tf.initializers.glorot_uniform())\n","b_g_5=tf.compat.v1.get_variable(\"biases5\", [2*gru_units], initializer=tf.constant_initializer(bias_gate))\n","\n","w_c_1=tf.compat.v1.get_variable('weights6', [1, 2],initializer=tf.initializers.glorot_uniform())\n","b_c_1=tf.compat.v1.get_variable(\"biases6\", [2], initializer=tf.constant_initializer(bias_candidate))\n","w_c_2=tf.compat.v1.get_variable('weights7', [2,4], initializer=tf.initializers.glorot_uniform())\n","b_c_2=tf.compat.v1.get_variable(\"biases7\", [4], initializer=tf.constant_initializer(bias_candidate))\n","w_c_3=tf.compat.v1.get_variable('weights8', [5,8], initializer=tf.initializers.glorot_uniform())\n","b_c_3=tf.compat.v1.get_variable(\"biases8\", [8], initializer=tf.constant_initializer(bias_candidate))\n","w_c_4=tf.compat.v1.get_variable('weights9', [8, 16],initializer=tf.initializers.glorot_uniform())\n","b_c_4=tf.compat.v1.get_variable(\"biases9\", [16], initializer=tf.constant_initializer(bias_candidate))\n","w_c_5=tf.compat.v1.get_variable('weights10', [21+gru_units, gru_units], initializer=tf.initializers.glorot_uniform())\n","b_c_5=tf.compat.v1.get_variable(\"biases10\", [gru_units], initializer=tf.constant_initializer(bias_candidate))\n","\n","model_name = 'tgcn'\n","if model_name == 'tgcn':\n","    pred,ttts,ttto,jj= TGCN(inputs, weights, biases)\n","y_pred = pred\n","\n","###### optimizer ######\n","lambda_loss = 0.0015\n","Lreg = lambda_loss * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.compat.v1.trainable_variables())\n","label = tf.reshape(labels, [-1,num_nodes])\n","loss = tf.reduce_mean(tf.nn.l2_loss(y_pred-label) + Lreg)\n","error = tf.sqrt(tf.reduce_mean(tf.square(y_pred-label)))\n","optimizer = tf.compat.v1.train.AdamOptimizer(lr).minimize(loss)\n","\n","###### Initialize session ######\n","variables =tf.compat.v1.global_variables()\n","saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\n","gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.333)\n","sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n","sess.run(tf.compat.v1.global_variables_initializer())\n","from tqdm import tqdm\n","import pandas as pd\n","df=pd.DataFrame([],columns=[\"TRAIN_RMSE\",'TEST_RMSE',\"TEST_MAE\",\"ACCURACY\"])\n","path=\"/content/drive/MyDrive/CONVERGENCE_STUDY\"\n","\n","def evaluation(a,b):\n","    rmse = math.sqrt(mean_squared_error(a,b))\n","    mae = mean_absolute_error(a, b)\n","    mape = mean_absolute_percentage_error(a,b)*100\n","    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n","    r2 = 1-((a-b)**2).sum()/((a-a.mean())**2).sum()\n","    var = 1-(np.var(a-b))/np.var(a)\n","    return rmse, mae,mape,1-F_norm, r2, var\n","\n","\n","x_axe,batch_loss,batch_rmse,batch_pred = [], [], [], []\n","test_loss,test_rmse,test_mae,test_acc,test_r2,test_var,test_pred = [],[],[],[],[],[],[]\n","test_mape=[]\n","high_acc=0\n","low_rmse=0\n","high_mape=999999999999999999999\n","batch_mae=[]\n","batch_mape=[]\n","for epoch in range(training_epoch):\n","    for m in tqdm(range(totalbatch)):\n","        mini_batch = trainX[m * batch_size : (m+1) * batch_size]\n","        mini_label = trainY[m * batch_size : (m+1) * batch_size]\n","        _, loss1, rmse1,train_output = sess.run([optimizer, loss, error, y_pred],\n","                                                 feed_dict = {inputs:mini_batch, labels:mini_label})\n","\n","        train_label = np.reshape(mini_label,[-1,num_nodes])\n","        rmse1, mae1,mape1, acc1, r2_score1, var_score1 = evaluation(train_label, train_output )\n","        batch_loss.append(loss1)\n","        batch_rmse.append(rmse1 * max_value)\n","        batch_mae.append(mae1* max_value)\n","        batch_mape.append(mape1)\n","\n","     # Test completely at every epoch\n","    loss2, rmse2, test_output = sess.run([loss, error, y_pred],\n","                                         feed_dict = {inputs:testX, labels:testY})\n","    test_label = np.reshape(testY,[-1,num_nodes])\n","    rmse, mae,mape, acc, r2_score, var_score = evaluation(test_label, test_output)\n","    test_label1 = test_label * max_value\n","    test_output1 = test_output * max_value\n","    test_loss.append(loss2)\n","    test_rmse.append(rmse * max_value)\n","    test_mae.append(mae * max_value)\n","    test_acc.append(acc)\n","    test_r2.append(r2_score)\n","    test_var.append(var_score)\n","    test_pred.append(test_output1)\n","    test_mape.append(mape)\n","    np.save(\"T.npy\",test_mape)\n","    print('Iter:{}'.format(epoch),\n","          'train_rmse:{:.4}'.format(batch_rmse[-1]),\n","          'train_loss:{:.4}'.format(batch_loss[-1]),\n","          'test_loss:{:.4}'.format(loss2),\n","          'test_rmse:{:.4}'.format(rmse * max_value),\n","          'train_mae:{:.4}'.format(batch_mae[-1]),\n","          'test_mae:{:.4}'.format(mae * max_value),\n","          'train_mape:{:.4}'.format(batch_mape[-1]),\n","          'test_mape:{:.4}'.format(mape))\n","    df2=pd.DataFrame([[round(batch_loss[-1],4),round(loss2,4),round(batch_rmse[-1],4),round(rmse * max_value,4),round(mae * max_value,4),round(mape,4),round(batch_mae[-1],4),round(batch_mape[-1],4)]],columns=[\"TRAIN_LOSS\",\"TEST_LOSS\",\"TRAIN_RMSE\",'TEST_RMSE',\"TEST_MAE\",\"TEST_MAPE\",\"TRAIN_MAE\",\"TRAIN_MAPE\"])\n","    df=pd.concat([df,df2],ignore_index=True)\n","    df.to_csv(f\"/content/drive/MyDrive/HYPERPARAMTER/atttention_heads/MTERLA_0.2_{pre_len}_{heads}.csv\",index=False)\n","    if test_mape[-1]<high_mape:\n","      print(f\"test mape improved from {high_mape} to {test_mape[-1]}\")\n","      high_mape=test_mape[-1]\n","      saver.save(sess, path+'/model_100/TGCN_pre_%r'%epoch, global_step = epoch)\n","    else:\n","      print(\"NOT IMPROVED\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BDGuIb2GVKb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sf5_xcJENNzd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cp2dN6fFNOHT"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}